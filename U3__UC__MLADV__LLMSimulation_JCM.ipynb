{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC6nZBH/lBP3ZsAZpsDACc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcmachicao/MachineLearningAvanzado_UC_2024/blob/main/UC__MLADV__LLMSimulation_JCM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Rbn59PnrNCBd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "    # Compute the dot product of query and key\n",
        "    dot_product = np.dot(query, key.T)\n",
        "\n",
        "    # Scale the dot product\n",
        "    scaling_factor = np.sqrt(query.shape[-1])\n",
        "    scaled_dot_product = dot_product / scaling_factor\n",
        "\n",
        "    # Compute the attention weights\n",
        "    attention_weights = softmax(scaled_dot_product)\n",
        "\n",
        "    # Compute the weighted sum of values\n",
        "    attended_values = np.dot(attention_weights, value)\n",
        "\n",
        "    return attention_weights, attended_values\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / exp_x.sum(axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "7Qph1CfnNGDc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input paragraph represented as embeddings\n",
        "paragraph = np.array([\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [0.4, 0.5, 0.6],\n",
        "    [0.7, 0.8, 0.9]\n",
        "])"
      ],
      "metadata": {
        "id": "z7iHRAFfNLL2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the embeddings into heads (assuming 2 heads for simplicity)\n",
        "num_heads = 2\n",
        "head_size = paragraph.shape[-1] // num_heads\n",
        "query_heads = np.array_split(paragraph, num_heads, axis=-1)\n",
        "key_heads = np.array_split(paragraph, num_heads, axis=-1)\n",
        "value_heads = np.array_split(paragraph, num_heads, axis=-1)\n",
        "\n",
        "# Perform multihead attention\n",
        "output_heads = []\n",
        "attention_weights_heads = []\n",
        "for i in range(num_heads):\n",
        "    attention_weights, attended_values = scaled_dot_product_attention(query_heads[i], key_heads[i], value_heads[i])\n",
        "    output_heads.append(attended_values)\n",
        "    attention_weights_heads.append(attention_weights)\n",
        "\n",
        "# Concatenate the outputs from different heads\n",
        "output = np.concatenate(output_heads, axis=-1)\n",
        "attention_weights = np.concatenate(attention_weights_heads, axis=0)"
      ],
      "metadata": {
        "id": "3Nx3Wu1YNOn-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "__Xla1vGho2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be0d1f2c-7790-4b0d-af6e-16a68e6da4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Paragraph:\n",
            "[[0.1 0.2 0.3]\n",
            " [0.4 0.5 0.6]\n",
            " [0.7 0.8 0.9]]\n",
            "\n",
            "Multihead Attended Output:\n",
            "[[0.41271934 0.51271934 0.61797574]\n",
            " [0.43795362 0.53795362 0.63580695]\n",
            " [0.46258873 0.56258873 0.6533541 ]]\n",
            "\n",
            "Attention Weights:\n",
            "[[0.31235921 0.33288379 0.354757  ]\n",
            " [0.27208401 0.32931993 0.39859606]\n",
            " [0.2345047  0.32236152 0.44313378]\n",
            " [0.30382285 0.33243515 0.36374199]\n",
            " [0.2754406  0.32976227 0.39479712]\n",
            " [0.24838726 0.32537849 0.42623425]]\n"
          ]
        }
      ],
      "source": [
        "# Print the results\n",
        "print(\"Original Paragraph:\")\n",
        "print(paragraph)\n",
        "print(\"\\nMultihead Attended Output:\")\n",
        "print(output)\n",
        "print(\"\\nAttention Weights:\")\n",
        "print(attention_weights)"
      ]
    }
  ]
}
